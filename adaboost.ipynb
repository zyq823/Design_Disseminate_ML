{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adaboost",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "avyPQuDYT8Gw"
      },
      "source": [
        "#this first block loads several modules that we may or may not use\n",
        "#it serves as a check that you have them installed\n",
        "\n",
        "from math import floor, sqrt, pi\n",
        "from random import sample\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plot\n",
        "plot.rcParams[\"figure.figsize\"] = (20, 10) # (w, h)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#scikit image libs\n",
        "from skimage import io, exposure\n",
        "from skimage.util import img_as_float\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.filters import frangi, meijering, prewitt, gabor\n",
        "from skimage.feature import hog\n",
        "\n",
        "#pathing libraries\n",
        "import os\n",
        "import glob\n",
        "\n",
        "#loading images from Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Modules loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFWuby8M1cFY"
      },
      "source": [
        "#prepares n subplot boxes\n",
        "def npaxes(n):\n",
        "    #\n",
        "    r = floor(sqrt(n))\n",
        "    while(n % r != 0):\n",
        "        r -= 1\n",
        "    c = int(n / r)\n",
        "    #\n",
        "    fig, ax = plot.subplots(r, c)\n",
        "    r_ = 0\n",
        "    c_ = 0\n",
        "\n",
        "    axes = []\n",
        "\n",
        "    for _ in range(n):\n",
        "        if(c == 1 or r == 1):\n",
        "          axes.append(ax[max(r_, c_)])\n",
        "        else:\n",
        "          axes.append(ax[r_, c_])\n",
        "        #\n",
        "        c_ += 1\n",
        "        if(c_ >= c):\n",
        "            c_ = 0\n",
        "            r_ += 1\n",
        "\n",
        "    #plot.tight_layout()\n",
        "    #plot.show()\n",
        "\n",
        "    return axes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3_2htdfWFTh"
      },
      "source": [
        "#Access to Google Drive content\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UVwca-nWGzZ"
      },
      "source": [
        "#basic directory information\n",
        "\n",
        "basePath = '/content/drive/My Drive/CSCE633HW5'\n",
        "\n",
        "print(os.listdir(basePath))\n",
        "\n",
        "train_dir = basePath + '/train_PREPROCESSED/' #destination of where images will be saved\n",
        "test_dir = basePath + '/test_PREPROCESSED/' #destination of where images will be saved"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk0KzhBCWHXx"
      },
      "source": [
        "#file listing and  loading\n",
        "\n",
        "def test_extension(f):\n",
        "  extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
        "  ftitle, fext = os.path.splitext(os.path.basename(f))\n",
        "  for ext in extensions:\n",
        "    if(fext == ext):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "#gets files in the given directory that match our extensions (images)\n",
        "def get_files(dir):\n",
        "  return [f for f in os.listdir(dir) if test_extension(f)]\n",
        "\n",
        "#opens the image file as an skimage\n",
        "def get_image(dir, f):\n",
        "  return img_as_float(io.imread(os.path.join(dir, f)))\n",
        "\n",
        "def normalize(img):\n",
        "  return (img - img.mean()) / img.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIdlCxCAgYVP"
      },
      "source": [
        "img = get_image(train_dir, get_files(train_dir)[0])\n",
        "\n",
        "def get_random_img(dir):\n",
        "  return get_image(dir, sample(get_files(dir), 1)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as7DJ-XjjK9F"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z-t3cm8FYA1"
      },
      "source": [
        "Feature Extraction and Visualization begins below.\n",
        "\n",
        "Feature 1: Image mean and stantdard deviation, no visualization (just print the tuple)\n",
        "\n",
        "Feature 2: gabor filters (horizontal and vertical) and prewitt edge detector, visualization through filtered image plot + histogram of filtered image values (which may also be useful for processing)\n",
        "\n",
        "Feature 3: Histogram of Oriented Gradients (HOG), visualized through the scikit visualization support; the actual data comes out as a giant feature vector, ready for further processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpYZXRNMVzAv"
      },
      "source": [
        "#feature 1: image mean and stdev (come shipped with scikit, yay!)\n",
        "#while we often want to normalize images,\n",
        "#it may be useful in this context to keep the mean/stdev\n",
        "#(apparently the lightness/darkness of the lungs in x-rays can indicate their health)\n",
        "#we can then normalize the images after extracting these features\n",
        "\n",
        "def feature1(img):\n",
        "  return (img.mean(), img.std())\n",
        "\n",
        "def feature1_display(img):\n",
        "  print(feature1(img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HZcmppqzZp0"
      },
      "source": [
        "feature1_display(get_random_img(train_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1e79gLbJli"
      },
      "source": [
        "#feature 2: a couple scikit-supported filters\n",
        "\n",
        "def feature2(img):\n",
        "  img = rgb2gray(img)\n",
        "  g1 = gabor(img, 1, theta = 0)\n",
        "  g2 = gabor(img, 1, theta = pi / 2)\n",
        "  return (\n",
        "  #first: gabor filters\n",
        "  #these are basically edge detectors (or \"orientation\" detectors)\n",
        "  #these are an essential part of every visual feature kit :^)\n",
        "  #scikit returns \"real and imaginary responses\"\n",
        "    g1[0],\n",
        "    g2[0],\n",
        "  #edge filter\n",
        "    prewitt(img)\n",
        "  #a filter for branching shapes\n",
        "  #neither this nor the meijering filter worked well - maybe one of you can tinker and get it to do something interesting?\n",
        "    #frangi(img, black_ridges = False)\n",
        "  #second: meijering filter, a cool filter I found for \"neuriteness\" - branching shapes\n",
        "  #which should highlight the internal structure of the lungs\n",
        "    #meijering(img, black_ridges = False)\n",
        "  )\n",
        "\n",
        "#the way we'd use these filters, other than plugging them into a NN,\n",
        "#is probably to compute their means or counts - this will give us a general notion\n",
        "#of the \"amount\" of the input image that matches that filter\n",
        "#or, similar to the HOG below, we can create histograms of the values in the image after application of the filters\n",
        "\n",
        "#returns (hist, bin_edges)\n",
        "#see https://numpy.org/doc/stable/reference/generated/numpy.histogram.html\n",
        "def feature2_histogram(feat):\n",
        "  return np.histogram(feat, 'sqrt')\n",
        "\n",
        "  #this function displays the results of feature-extraction 2\n",
        "#i.e.: a picture of the image along with the result after the image goes through each filter\n",
        "#as well as histograms calculated over the values in each picture\n",
        "def feature2_display(img):\n",
        "  fs = (rgb2gray(img),) + feature2(img)\n",
        "  n = len(fs)\n",
        "\n",
        "  axes = npaxes(n * 2)\n",
        "\n",
        "  for i in range(n):\n",
        "    f = fs[i]\n",
        "    axes[i].imshow(f, cmap=\"gray\")\n",
        "    axes[i + n].hist(f.flatten(), bins=\"sqrt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihqv5XRmC2xO"
      },
      "source": [
        "#display extracted features from a random training image\n",
        "feature2_display(get_random_img(train_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfOCzJlEcKNn"
      },
      "source": [
        "#feature 3: HOG (histogram of oriented gradients)\n",
        "#this extracts a huge amount of information about the orientation of edges in the picture\n",
        "#and returns a giant feature vector.\n",
        "#it has several options, but the defaults should be good enough for us (we can test this).\n",
        "#the two options we are most likely to modify are pixels_per_cell and cells_per_block, which control the way that HOG splits up the image.\n",
        "\n",
        "#this returns a feature vector\n",
        "#it is not suitable for visualization\n",
        "def feature3(img):\n",
        "  return hog(img, multichannel=True)\n",
        "\n",
        "#this plots the corresponding HOG image\n",
        "def feature3_display(img):\n",
        "  fd, hog_img = hog(img, multichannel = True, visualize=True)\n",
        "  hog_img = exposure.rescale_intensity(hog_img, in_range=(0, 10))\n",
        "  plot.imshow(hog_img, cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGdJN7rIDU3b"
      },
      "source": [
        "feature3_display(get_random_img(train_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOwTwuXUeLT-"
      },
      "source": [
        "## (iii) \n",
        "\n",
        "1-Feature selection: \n",
        "Explore two different feature selection methods of your choice. One method should be part of the Filter category and the other should be part of the Wrapper category\n",
        "\n",
        "2- Classification: \n",
        "Using a simple classifier (e.g., SVM, logistic regression), plot the classification performance using a 5-fold cross-validation on the training data against the number of features for both feature selection methods. Compare and contrast between the two (e.g., in terms of performance and computation time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdiEHJ03wE7k"
      },
      "source": [
        "#Load data\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/My Drive/CSCE633HW5/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/My Drive/CSCE633HW5/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FG0m2eKx2L3"
      },
      "source": [
        "train_data.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nTkmKEVyQfN"
      },
      "source": [
        "#get the imags and labels \n",
        "\n",
        "def get_imgs(dataframe,dir):\n",
        "  imgs = []\n",
        "  for i in range(0,len(dataframe)):\n",
        "      ig = get_image(dir, dataframe.iloc[i]['filename']) #match the order of the images in the dataframe and train_dir \n",
        "      imgs.append(ig)                                           #add all the images in a big array \n",
        "  return np.array(imgs,dtype='float32')\n",
        "\n",
        "train_imgs = get_imgs(train_data,train_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILVb-_qGsiYu"
      },
      "source": [
        "train_imgs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FhD1gyKrS_-"
      },
      "source": [
        "#get the labels (outcome) as an array \n",
        "def get_labels(dataframe):\n",
        "  labels = dataframe['covid(label)']\n",
        "  return labels.to_numpy()\n",
        "y = get_labels(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtIcF8KFl4oY"
      },
      "source": [
        "#get the features for a given imgs \n",
        "def get_features(feature,imgs):\n",
        "  train_data1 = []\n",
        "  for i in range(0,len(imgs)):\n",
        "    train_data1.append(feature(imgs[i]))\n",
        "  return np.array(train_data1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK8xV_oBdgi-"
      },
      "source": [
        "#get feature 1 for all images \n",
        "f1 = get_features(feature1,train_imgs)\n",
        "\n",
        "#get feature 2 for all images\n",
        "f2 = get_features(feature2,train_imgs)\n",
        "f2_flatten = f2.reshape((-1, 200*200*3)) #flatten feature2 output to 1D array \n",
        "\n",
        "#get feature 3 for all images \n",
        "f3 = get_features(feature3,train_imgs)\n",
        "\n",
        "#combine the three features in one big feature matrix in which each column represent a potential feature \n",
        "X = np.concatenate((f1,f2_flatten,f3),axis=1)\n",
        "\n",
        "print(f\"The size of feature 1 array is {f1.shape}\")\n",
        "print(f\"The size of feature 2 array is {f2_flatten.shape}\")\n",
        "print(f\"The size of feature 3 array is {f3.shape}\")\n",
        "print(f\"the size of the feature matrix is {X.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kZmNfnr4n65"
      },
      "source": [
        "Quick intermission for feature scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mdEv5-i-E-l"
      },
      "source": [
        "#only run this command if the next cell fails with \"module not found\" on skfeature\n",
        "#!pip install git+https://github.com/jundongl/scikit-feature.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELptjOTh4m37"
      },
      "source": [
        "#TODO: fisher score, gini index, conditional entropy...\n",
        "\n",
        "#gotta love Python and the hard work everybody else put into this stuff\n",
        "from skfeature.function.similarity_based.fisher_score import fisher_score, feature_ranking as rank_fisher\n",
        "from skfeature.function.statistical_based.gini_index import gini_index, feature_ranking as rank_gini\n",
        "from skfeature.utility.mutual_information import conditional_entropy\n",
        "\n",
        "#fisher score\n",
        "fisher_scores = fisher_score(X, y) #score for each feature\n",
        "fisher_rank = rank_fisher(fisher_scores) #feature indices sorted by fisher score\n",
        "fisher_n = 10\n",
        "print(\"Top\", fisher_n, \"features (fisher):\", fisher_rank[:fisher_n])\n",
        "\n",
        "#gini index\n",
        "gini_indices = gini_index(X, y)\n",
        "gini_rank = rank_gini(gini_indices)\n",
        "gini_n = 10\n",
        "print(\"Top\", gini_n, \"features (gini):\", gini_rank[:gini_n])\n",
        "\n",
        "#conditional entropy\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EePhqc3tEbrg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gN3Ktp6rOxD"
      },
      "source": [
        "Back to feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oNmY239ez7L"
      },
      "source": [
        "#Filter Method\n",
        "#use SelectKbest from sklearn that takes the matrix features, the labels\n",
        "#and number of features to select (k) then performs statistical tests to\n",
        "#select k number of features \n",
        "#the number of features targeted is 50\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "skb = SelectKBest(chi2, k=50)\n",
        "X_filter = skb.fit_transform(X, y)\n",
        "print(skb.get_support(indices = True)) #this prints the indices of the selected features "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtjedtnBDg27"
      },
      "source": [
        "Feature Selection method 2 : wrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEdpuQ82Dmzl"
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxrLFe2FShGB"
      },
      "source": [
        "#The wrapper method used is the Recursive Feature Elimination (RFE) \n",
        "#it works by recursively removing features and building a model on the remaining\n",
        "#this method would go over all the features (162,000) and as a result takes a lot of time to run\n",
        "#A way to lower the computational time for the wrapper method is to use \n",
        "#the filter method initially and apply it to get 2000 feature out of the 162,000\n",
        "#then the REF is applied to the 1000 features to select 50 features\n",
        "\n",
        "\n",
        "#Filter method \n",
        "skb2 = SelectKBest(chi2, k=1000)\n",
        "X_filter2 = skb2.fit_transform(X, y)\n",
        "# print(skb2.get_support(indices = True))\n",
        "\n",
        "#Wrapper method \n",
        "\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
        "rfe = RFE(model, 50)\n",
        "X_wrapper2 = rfe.fit_transform(X_filter2, y)\n",
        "print(skb2.get_support(indices = True)[rfe.get_support(indices = True)]) #this prints the indicies of the selected features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvrFEHUWVxBd"
      },
      "source": [
        "Apply logistic regression to a 5-fold cross-validation on the training data across the features selected with the filter method and the wrapper method features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwSPib-1WDL-"
      },
      "source": [
        "def cross_val(X,y):\n",
        "  random_index = np.random.permutation(len(X))\n",
        "  scores = []\n",
        "  for i in range(0,5):\n",
        "      #initilize the training/testing lists\n",
        "      Xs = []\n",
        "      Xt = []\n",
        "      ys = []\n",
        "      yt = []\n",
        "      #split into 5 folds randomly\n",
        "      for j in range(0,len(X)): \n",
        "          if (j%5 ==i):\n",
        "              Xs.append(X[random_index[j]])\n",
        "              ys.append(y[random_index[j]])\n",
        "          else: \n",
        "              Xt.append(X[random_index[j]])\n",
        "              yt.append(y[random_index[j]])\n",
        "      #convert list to array\n",
        "      Xs = np.asarray(Xs)\n",
        "      Xt = np.asarray(Xt)\n",
        "      ys = np.asarray(ys)\n",
        "      yt = np.asarray(yt)\n",
        "      \n",
        "      #apply logistic regression to the testing data\n",
        "      model = LogisticRegression(solver='saga', max_iter=10000)\n",
        "      #C = Inverse of regularization strength; must be a positive float --> smaller values specify stronger regularization.\n",
        "      model.fit(Xt, yt)\n",
        "      pred = model.predict(Xs)\n",
        "      score = model.score(Xs, ys)\n",
        "      scores += [score]\n",
        "      print((round(score,3)))\n",
        "\n",
        "      # plot the performance \n",
        "      plot.plot(range(1, len(scores)+1), scores)\n",
        "      plot.yticks([0, 0.5, 1])\n",
        "      plot.xticks([0, 1, 2, 3, 4, 5])\n",
        "      plot.xlabel(\"Subset of features\", fontsize = 25)\n",
        "      plot.ylabel(\"Cross validation score \", fontsize =25)\n",
        "      # plot.show()\n",
        "  print(\"Mean score:\", sum(scores) / len(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-NUmUryzO_M"
      },
      "source": [
        "#apply cross validation to filter method features\n",
        "cross_val(X_filter,y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTRzU_Frzqx5"
      },
      "source": [
        "#apply the cross validation to the wrapper method features\n",
        "cross_val(X_wrapper2,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0zykC_hXIbc"
      },
      "source": [
        "b.iv: Adaboost\n",
        "\n",
        "Learn a simple model (in this case, logistic regression) multiple times, each time weighting the samples based on how badly the *previous* models did on them; each new model is weighted by how well it does"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTwqg1wkXJQR"
      },
      "source": [
        "#X and y should already be defined from feature selection above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAjHEJCmbtha"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "\n",
        "def adaboost(T, cv_count = 5):\n",
        "  classifier = AdaBoostClassifier(LogisticRegression(max_iter=10000), n_estimators=T)\n",
        "  scores = cross_validate(classifier, X, y, cv=cv_count, scoring = make_scorer(accuracy_score))[\"test_score\"]\n",
        "  print(\"AdaBoost(\", T, \") test scores:\", scores)\n",
        "  print(\"Average:\", sum(scores)/len(scores))\n",
        "\n",
        "adaboost(1)\n",
        "adaboost(10)\n",
        "adaboost(100)\n",
        "adaboost(1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv_1Ic_Cpl7i"
      },
      "source": [
        "AdaBoost only performs about as well as the \"filter\" method, and is *significantly* more expensive (in terms of runtime)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6jXcppEp4t6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}