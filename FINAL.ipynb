{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7ETuMcETgG4"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZuScltrTdVU"
      },
      "source": [
        "#only run this if the next cell fails with \"module not found\" on skfeature\n",
        "!pip install git+https://github.com/jundongl/scikit-feature.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6K7HFMTid5"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqV9kLlLTe4J"
      },
      "source": [
        "#basics\n",
        "from math import floor, sqrt, pi\n",
        "from random import sample\n",
        "import time\n",
        "import timeit\n",
        "#helpful progress bar\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "#file access\n",
        "import os\n",
        "import glob\n",
        "from google.colab import drive\n",
        "\n",
        "#data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#libraries for image trimming and resizing\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "\n",
        "#matplot\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plot\n",
        "plot.rcParams[\"figure.figsize\"] = (20, 10) # (w, h)\n",
        "\n",
        "#scikit\n",
        "#gotta love Python and the hard work everybody else put into this stuff\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from skimage import io, exposure\n",
        "from skimage.util import img_as_float\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.filters import frangi, meijering, prewitt, gabor\n",
        "from skimage.feature import hog\n",
        "from skfeature.function.similarity_based.fisher_score import fisher_score, feature_ranking as rank_fisher\n",
        "from skfeature.function.statistical_based.gini_index import gini_index, feature_ranking as rank_gini\n",
        "from skfeature.utility.mutual_information import conditional_entropy\n",
        "#torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "\n",
        "#Done\n",
        "print(\"Modules loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV0zUHAlTm0V"
      },
      "source": [
        "## File Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ctbBFdZTn6B"
      },
      "source": [
        "#Access to Google Drive content\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#basic directory information\n",
        "basePath = '/content/drive/My Drive/CSCE633HW5/'\n",
        "print(os.listdir(basePath))\n",
        "#directory structure for the project\n",
        "train_dir_raw = basePath + 'train/'\n",
        "test_dir_raw = basePath + 'test/'\n",
        "train_dir = basePath + 'train_PREPROCESSED/' #destination of where images will be saved\n",
        "test_dir = basePath + 'test_PREPROCESSED/' #destination of where images will be saved\n",
        "train_dir_cnn = basePath + 'train_CNN/'\n",
        "test_dir_cnn = basePath + 'test_CNN/'\n",
        "#data files\n",
        "train_data = pd.read_csv(basePath + 'train.csv')\n",
        "test_data = pd.read_csv(basePath + 'test.csv')\n",
        "\n",
        "#file listing and  loading\n",
        "def test_extension(f):\n",
        "  extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
        "  ftitle, fext = os.path.splitext(os.path.basename(f))\n",
        "  for ext in extensions:\n",
        "    if(fext == ext):\n",
        "      return True\n",
        "  return False\n",
        "#\n",
        "def normalize(img):\n",
        "  return (img - img.mean()) / img.std()\n",
        "#opens the image file as an skimage\n",
        "def get_image(directory, f):\n",
        "  return (img_as_float(io.imread(os.path.join(directory, f))))\n",
        "#load the images referenced in the given data file\n",
        "#this ensures they are in the same order\n",
        "def get_imgs(dataframe,directory):\n",
        "  imgs = []\n",
        "  for i in trange(len(dataframe)):\n",
        "      ig = get_image(directory, dataframe.iloc[i]['filename']) #match the order of the images in the dataframe and train_dir \n",
        "      imgs.append(ig)                                           #add all the images in a big array \n",
        "  return np.array(imgs,dtype='float32')\n",
        "#get the labels (outcome) as an array \n",
        "def get_labels(dataframe):\n",
        "  labels = dataframe['covid(label)']\n",
        "  return labels.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ufkYeWTvhL"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvsEaDpITwzi"
      },
      "source": [
        "def crop_center(pil_img, crop_width, crop_height): #manual definition of crop_center\n",
        "    img_width, img_height = pil_img.size\n",
        "    return pil_img.crop(((img_width - crop_width) // 2,\n",
        "                         (img_height - crop_height) // 2,\n",
        "                         (img_width + crop_width) // 2,\n",
        "                         (img_height + crop_height) // 2))\n",
        "\n",
        "#Function for getting largest square from center (shaves off excess height, maintain current width)\n",
        "def crop_max_square(pil_img):\n",
        "    return crop_center(pil_img, min(pil_img.size), min(pil_img.size))\n",
        "\n",
        "#function that takes in source folder of images, destination folder to save, and image width\n",
        "#The for loop reads each image file, converts to RGB, resizes using LANCZOS filter and saves it to dst_dir\n",
        "def preprocess(src_dir,dst_dir,imageWidth):\n",
        "  files = [f for f in os.listdir(src_dir) if test_extension(f)]\n",
        "  print(\"Processing\", src_dir, \"->\", dst_dir, f\"({imageWidth}x{imageWidth})\")\n",
        "  for f in tqdm(files):\n",
        "    im = Image.open(os.path.join(src_dir, f))\n",
        "    im = im.convert(\"RGB\") # important, not all are JPEG. Converts from RGBA to JPEG compatible\n",
        "    im_thumb = crop_max_square(im).resize((imageWidth, imageWidth), Image.LANCZOS)\n",
        "    im_thumb.save(os.path.join(dst_dir, f), quality=95)\n",
        "  print(\"Processed\", len(files), \"files\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crabgy-xT6OS"
      },
      "source": [
        "preprocess(train_dir_raw, train_dir, 200)\n",
        "preprocess(test_dir_raw, test_dir, 200)\n",
        "preprocess(train_dir_raw, train_dir_cnn, 100)\n",
        "preprocess(test_dir_raw, test_dir_cnn, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs4gBcQdARSo"
      },
      "source": [
        "# Load the training files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kru4fvRWTszC"
      },
      "source": [
        "#load the datasets\n",
        "X_train_img = get_imgs(train_data, train_dir)\n",
        "#X_train = features(X_img_train)\n",
        "X_train_cnn = get_imgs(train_data, train_dir_cnn)\n",
        "y_train = get_labels(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DjAvTpxUG6C"
      },
      "source": [
        "# Feature Extraction and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmeJmwBTUJyE"
      },
      "source": [
        "#prepares n subplot boxes\n",
        "def npaxes(n):\n",
        "    #\n",
        "    r = floor(sqrt(n))\n",
        "    while(n % r != 0):\n",
        "        r -= 1\n",
        "    c = int(n / r)\n",
        "    #\n",
        "    fig, ax = plot.subplots(r, c)\n",
        "    r_ = 0\n",
        "    c_ = 0\n",
        "\n",
        "    axes = []\n",
        "\n",
        "    for _ in range(n):\n",
        "        if(c == 1 or r == 1):\n",
        "          axes.append(ax[max(r_, c_)])\n",
        "        else:\n",
        "          axes.append(ax[r_, c_])\n",
        "        #\n",
        "        c_ += 1\n",
        "        if(c_ >= c):\n",
        "            c_ = 0\n",
        "            r_ += 1\n",
        "\n",
        "    #plot.tight_layout()\n",
        "    #plot.show()\n",
        "\n",
        "    return axes\n",
        "\n",
        "#I wanted to use meijering neuriteness / frangi edge filters\n",
        "#but they didn't really work - prewitt filter seemed to extract edges much better\n",
        "\n",
        "#extracts the following features:\n",
        "#-two gabor filters\n",
        "#-prewitt edge filter\n",
        "#-HOG\n",
        "#also generates visualization data for each\n",
        "#so it returns a pair, first element = feature vector, second element = visualization stuff\n",
        "def extract_features(img):\n",
        "    img = rgb2gray(img)\n",
        "    g1 = gabor(img, 1, theta = 0)\n",
        "    g2 = gabor(img, 1, theta = pi / 2)\n",
        "    p = prewitt(img)\n",
        "    (hog_fd, hog_img) = hog(img, visualize=True)\n",
        "    hog_img = exposure.rescale_intensity(hog_img, in_range=(0, 10))\n",
        "    #\n",
        "    #features = [np.histogram(feat, bins='sqrt') for feat in [g1[0], g2[0], p]] + [hog_fd]\n",
        "    features = np.concatenate((g1[0].flatten(), g2[0].flatten(), p.flatten(), hog_fd))\n",
        "    visualizations = [img, g1[0], g2[0], p, hog_img]\n",
        "    labels = [\"Src\", \"Gabor1\", \"Gabor2\", \"Prewitt\", \"HOG\"]\n",
        "    return (features, (visualizations, labels))\n",
        "\n",
        "def display_features(vl):\n",
        "    (vis, labels) = vl\n",
        "    n = len(vis)\n",
        "    axes = npaxes(n * 2)\n",
        "    for i in range(n):\n",
        "        f = vis[i]\n",
        "        axes[i].imshow(f, cmap=\"gray\")\n",
        "        axes[i].set_xlabel(labels[i])\n",
        "        axes[i + n].hist(f.flatten(), bins=\"sqrt\")\n",
        "        axes[i + n].set_xlabel(labels[i])\n",
        "\n",
        "def features(imgs):\n",
        "    feats = []\n",
        "    vis = []\n",
        "    for img in tqdm(imgs, desc=\"Extracting features\"):\n",
        "        (feat, v) = extract_features(img)\n",
        "        feats.append(feat)\n",
        "        vis.append(v)\n",
        "    return (np.stack(feats), np.stack(vis)) #.array and .stack should do the same thing here, I believe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYhNF_V4USM6"
      },
      "source": [
        "(X_train, X_train_vis) = features(X_train_img)\n",
        "\n",
        "#visualize one random point\n",
        "display_features(X_train_vis[sample(range(X_train_vis.shape[0]), 1)[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXPpuKdSUXkX"
      },
      "source": [
        "## Feature Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y1gNqXPUZHK"
      },
      "source": [
        "#unfortunately, there isn't any way to add a progress bar to the skfeature methods.\n",
        "# :(\n",
        "\n",
        "#conditional entropy\n",
        "def all_entropies(X, y):\n",
        "  feats = X.shape[1] #number of features\n",
        "  entropies = [0] * feats\n",
        "  for f in trange(feats):\n",
        "    Xs = X[:,f]\n",
        "    entropies[f] = conditional_entropy(Xs, y)\n",
        "  return entropies\n",
        "\n",
        "#returns the indices of entries in ascending order\n",
        "def sort_ranks(xs):\n",
        "  n = len(xs)\n",
        "  #associate each with its index\n",
        "  ids = zip(xs, range(n))\n",
        "  #sort by value and then retrieve index\n",
        "  return [x[1] for x in sorted(ids, key = lambda t: t[0])]\n",
        "\n",
        "#entropy_scores = all_entropies(X_train, y_train)\n",
        "entropy_rank = sort_ranks(entropy_scores)\n",
        "entropy_n = 10\n",
        "print(\"Top\", entropy_n, \"features (entropy):\", entropy_rank[:entropy_n])\n",
        "\n",
        "#fisher score\n",
        "#fisher_scores = fisher_score(X_train, y_train) #score for each feature\n",
        "#fisher_rank = rank_fisher(fisher_scores) #feature indices sorted by fisher score\n",
        "fisher_n = 10\n",
        "print(\"Top\", fisher_n, \"features (fisher):\", fisher_rank[:fisher_n])\n",
        "\n",
        "#gini index (takes a long time to calculate)\n",
        "#gini_indices = gini_index(X_train, y_train)\n",
        "#gini_rank = rank_gini(gini_indices)\n",
        "gini_n = 10\n",
        "print(\"Top\", gini_n, \"features (gini):\", gini_rank[:gini_n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3U-n0xHJJ1C"
      },
      "source": [
        "#plot histograms of the distribution of each of the most important features\n",
        "\n",
        "def display_best_features(X, features):\n",
        "    n = len(features)\n",
        "    axes = npaxes(n)\n",
        "    for i in range(n):\n",
        "        f = features[i]\n",
        "        data = X[:,f]\n",
        "        axes[i].set_xlabel(\"Feature \" + str(f))\n",
        "        axes[i].hist(data, bins=\"sqrt\")\n",
        "\n",
        "#remove duplicates and sort (makes it back into a list)\n",
        "def get_best_features():\n",
        "  a = entropy_rank[:entropy_n]\n",
        "  b = fisher_rank[:fisher_n].tolist()\n",
        "  c = gini_rank[:gini_n].tolist()\n",
        "  return sorted(set(a + b + c))\n",
        "\n",
        "best_features = get_best_features()\n",
        "print(\"Best features:\", best_features)\n",
        "display_best_features(X_train, best_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4oXfrLlUdib"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swRcB4iRUgSi"
      },
      "source": [
        "#Filter Method\n",
        "#use SelectKbest from sklearn that takes the matrix features, the labels\n",
        "#and number of features to select (k) then performs statistical tests to\n",
        "#select k number of features \n",
        "#the number of features targeted is 50\n",
        "\n",
        "skb = SelectKBest(chi2, k=50)\n",
        "X_filter = skb.fit_transform(X_train, y_train)\n",
        "print(\"Filter-method feature indices:\", skb.get_support(indices = True)) #this prints the indices of the selected features\n",
        "\n",
        "#The wrapper method used is the Recursive Feature Elimination (RFE) \n",
        "#it works by recursively removing features and building a model on the remaining\n",
        "#this method would go over all the features (162,000) and as a result takes a lot of time to run\n",
        "#A way to lower the computational time for the wrapper method is to use \n",
        "#the filter method initially and apply it to get 2000 feature out of the 162,000\n",
        "#then the REF is applied to the 1000 features to select 50 features\n",
        "\n",
        "\n",
        "#Filter method \n",
        "skb2 = SelectKBest(chi2, k=1000)\n",
        "X_filter2 = skb2.fit_transform(X_train, y_train)\n",
        "#print(skb2.get_support(indices = True))\n",
        "\n",
        "#Wrapper method \n",
        "\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
        "rfe = RFE(model, 50)\n",
        "X_wrapper2 = rfe.fit_transform(X_filter2, y_train)\n",
        "print(\"Wrapper-method feature indices:\", skb2.get_support(indices = True)[rfe.get_support(indices = True)]) #this prints the indicies of the selected features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3c8Kv80kTij"
      },
      "source": [
        "#used to test each of the features\n",
        "def cross_val(X, y):\n",
        "  random_index = np.random.permutation(len(X))\n",
        "  scores = []\n",
        "  for i in range(0,5):\n",
        "      #initilize the training/testing lists\n",
        "      Xs = []\n",
        "      Xt = []\n",
        "      ys = []\n",
        "      yt = []\n",
        "      #split into 5 folds randomly\n",
        "      for j in range(0,len(X)): \n",
        "          if (j%5 ==i):\n",
        "              Xs.append(X[random_index[j]])\n",
        "              ys.append(y[random_index[j]])\n",
        "          else: \n",
        "              Xt.append(X[random_index[j]])\n",
        "              yt.append(y[random_index[j]])\n",
        "      #convert list to array\n",
        "      Xs = np.asarray(Xs)\n",
        "      Xt = np.asarray(Xt)\n",
        "      ys = np.asarray(ys)\n",
        "      yt = np.asarray(yt)\n",
        "      \n",
        "      #apply logistic regression to the testing data\n",
        "      model = LogisticRegression(solver='saga', max_iter=10000)\n",
        "      #C = Inverse of regularization strength; must be a positive float --> smaller values specify stronger regularization.\n",
        "      model.fit(Xt, yt)\n",
        "      pred = model.predict(Xs)\n",
        "      score = model.score(Xs, ys)\n",
        "      scores += [score]\n",
        "      print((round(score,3)))\n",
        "\n",
        "      # plot the performance \n",
        "      plot.plot(range(1, len(scores)+1), scores)\n",
        "      plot.yticks([0, 0.5, 1])\n",
        "      plot.xticks([0, 1, 2, 3, 4, 5])\n",
        "      plot.xlabel(\"Subset of features\", fontsize = 25)\n",
        "      plot.ylabel(\"Cross validation score \", fontsize =25)\n",
        "      # plot.show()\n",
        "  print(\"Mean score:\", sum(scores) / len(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEE_QqNAkUOa"
      },
      "source": [
        "#apply cross validation to filter method features\n",
        "cross_val(X_filter, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZgtMO7fkT8j"
      },
      "source": [
        "#apply the cross validation to the wrapper method features\n",
        "cross_val(X_wrapper2, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RcW-f4FUmyQ"
      },
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVqGG7-pUo4I"
      },
      "source": [
        "def adaboost(T, cv_count = 5):\n",
        "  classifier = AdaBoostClassifier(LogisticRegression(max_iter=10000), n_estimators=T)\n",
        "  scores = cross_validate(classifier, X_train, y_train, cv=cv_count, scoring = make_scorer(accuracy_score))[\"test_score\"]\n",
        "  print(\"AdaBoost(\", T, \") test scores:\", scores)\n",
        "  print(\"Average:\", sum(scores)/len(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKcJixrxUqww"
      },
      "source": [
        "adaboost(1)\n",
        "adaboost(10)\n",
        "adaboost(100)\n",
        "adaboost(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXpRyvqmUttb"
      },
      "source": [
        "# Improving Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtBxQpNshZyp"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "#Reference link for cross-validation with CNN: https://medium.com/@navmcgill/k-fold-cross-validation-in-keras-convolutional-neural-networks-835bed559d04\n",
        "def optimize_cnn(hyperparameter):\n",
        "  \n",
        "  layers = [Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], strides = hyperparameter['conv_stride'], activation=hyperparameter['activation'], input_shape=(100,100,3)), \n",
        "            Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], strides = hyperparameter['conv_stride'], activation=hyperparameter['activation']), \n",
        "            MaxPooling2D(pool_size=(2,2),padding='same'), Dropout(hyperparameter['dropout_prob']),\n",
        "            Conv2D(64, kernel_size=hyperparameter['conv_kernel_size'], strides = hyperparameter['conv_stride'], activation=hyperparameter['activation']),\n",
        "            Conv2D(64, kernel_size=hyperparameter['conv_kernel_size'], strides = hyperparameter['conv_stride'], activation=hyperparameter['activation']), \n",
        "            MaxPooling2D(pool_size=(2,2),padding='same'), Dropout(hyperparameter['dropout_prob']),\n",
        "            Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], strides = hyperparameter['conv_stride'], activation=hyperparameter['activation']),\n",
        "            Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], strides = hyperparameter['conv_stride'], activation=hyperparameter['activation']),\n",
        "            MaxPooling2D(pool_size=(2,2),padding='same'), Dropout(hyperparameter['dropout_prob']),\n",
        "            Flatten(),\n",
        "            Dense(512,activation='relu'), #dense layer 1\n",
        "            Dense(256,activation='relu'), #dense layer 2\n",
        "            Dense(2,activation='softmax'), #dense layer 3\n",
        "            ] \n",
        "\n",
        "  # Define model using hyperparameters \n",
        "  used_layers = [layers[i] for i in hyperparameter['layer_ids']]+layers[12:]\n",
        "  cnn_model = Sequential(used_layers)\n",
        "  #print(cnn_model.summary())\n",
        "  cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  v_scores = []\n",
        "  t_scores = []\n",
        "  start_time = time.time()\n",
        "  for i in range(5):\n",
        "    tqdm_callback = tfa.callbacks.TQDMProgressBar()\n",
        "    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(X_train_cnn, y_train, test_size=0.20, random_state = np.random.randint(1,1000, 1)[0])\n",
        "    results = cnn_model.fit(X_train_fold, to_categorical(y_train_fold), validation_data=(X_val_fold, to_categorical(y_val_fold)), epochs=30, batch_size=52, verbose=0)\n",
        "    t_performance = results.history['accuracy'][-1]\n",
        "    _, v_performance = cnn_model.evaluate(X_val_fold, to_categorical(y_val_fold), verbose = 0)\n",
        "    v_scores.append(v_performance)\n",
        "    t_scores.append(t_performance)\n",
        "  end_time = time.time()\n",
        "  print(\"Training accuracy: {}\\nTraining time: {}\".format(sum(t_scores)/len(t_scores), end_time-start_time))\n",
        "\n",
        "  # Evaluate accuracy on validation data\n",
        "  #performance = cnn_model.evaluate(fnn_val_proc, to_categorical(val_labels), verbose=0)\n",
        "  averagePerformance = sum(v_scores)/len(v_scores)\n",
        "  print(\"Hyperparameters: \", hyperparameter, \"Validation Accuracy: \", averagePerformance)\n",
        "  print(\"----------------------------------------------------\")\n",
        "  # We want to minimize loss i.e. negative of accuracy\n",
        "  return({\"status\": STATUS_OK, \"loss\": -1*averagePerformance, \"model\":cnn_model,\"params\":hyperparameter,\"acc\":averagePerformance})\n",
        "\n",
        "# Define search space for hyper-parameters\n",
        "space = {\n",
        "    'layer_ids': hp.choice('layer_ids',[[0,2,3,4,6,7],[0,1,2,3],[0,2,3,8,10,11],[0,2,3]]),\n",
        "    'conv_kernel_size': hp.choice('conv_kernel_size',[1,3,5]),\n",
        "    'conv_stride': hp.choice('conv_stride',[1,2]),\n",
        "    'activation': hp.choice('activation', ['relu','selu','tanh']),\n",
        "    'dropout_prob': hp.uniform('dropout_prob',0,0.35),\n",
        "}\n",
        "\n",
        "trials = Trials()\n",
        "with tf.device('/device:GPU:0'):\n",
        "  # Find the best hyperparameters\n",
        "  best = fmin(\n",
        "          optimize_cnn,\n",
        "          space,\n",
        "          algo=tpe.suggest,\n",
        "          trials=trials,\n",
        "          max_evals=10,\n",
        "          verbose=0,\n",
        "          show_progressbar=False\n",
        "      )\n",
        "  \n",
        "  test_result = trials.results[np.argmin([r['loss'] for r in trials.results])] #best trial info\n",
        "  best_model = test_result['model']\n",
        "  best_params = test_result['params']\n",
        "  best_acc = test_result['acc']\n",
        "\n",
        "  print(\"\\n==================================\\n\")\n",
        "  print(\"Best Hyperparameters:\", best_params) #the output here doesn't makes sense because it prints indices into the hp.choice objects\n",
        "  print(\"Accuracy:\", best_acc)\n",
        "\n",
        "  # You can retrain the final model with optimal hyperparameters on train+validation data\n",
        "\n",
        "  # Or you can use the model returned directly\n",
        "  # Find trial which has minimum loss value and use that model to perform evaluation on the test data\n",
        "  #test_model = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n",
        "\n",
        "  #for bonus question if we have time :)\n",
        "  #performance = test_model.evaluate(fnn_test_proc, to_categorical(test_labels))\n",
        "\n",
        "  #print(\"==================================\")\n",
        "  #print(\"Test Accuracy: \", performance[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KXSsYYd6zbR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}